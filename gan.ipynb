{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, str_title):\n",
    "    #images는 무조건 n^2으로 가정\n",
    "    num_images = images.shape[0]\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    fig.suptitle(str_title, fontsize=24)\n",
    "\n",
    "    for ind in range(0, num_images):\n",
    "        plt.subplot(np.sqrt(num_images),\n",
    "                    np.sqrt(num_images), ind + 1)\n",
    "        plt.imshow(images[ind, :, :], cmap='gray')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_displays_per_row = 5\n",
    "num_displays = num_displays_per_row**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train[..., np.newaxis] # 이게 더 빠름\n",
    "# X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "size_batch = 64\n",
    "rate_learning_generator = 1e-4\n",
    "rate_learning_discriminator = 1e-4\n",
    "dim_noise = 32 #G 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "dataset_train = dataset_train.shuffle(10000).batch(size_batch)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "dataset_test = dataset_test.batch(size_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(7*7*256, input_shape = (dim_noise, )),  #(bs, 7*7*256)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Reshape((7, 7, 256)), #(bs, 7, 7, 256)\n",
    "    tf.keras.layers.Conv2DTranspose(128, (5, 5), strides = (1, 1), padding = 'same'), #(bs, 7, 7, 128)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Conv2DTranspose(64, (5, 5), strides = (2, 2), padding = 'same'), #(bs, 14, 14, 64)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Conv2DTranspose(1, (5, 5), strides = (2, 2), padding = 'same', activation = tf.nn.tanh)\n",
    "    \n",
    "])\n",
    "\n",
    "model_discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (5, 5), strides = (2, 2), padding = 'same', input_shape = [28, 28, 1]), #(bs, 14, 14, 64)\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv2D(128, (5, 5), strides = (2, 2), padding = 'same'), #(bs, 7, 7, 128)\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Flatten(), #(bs, 7 * 7 * 128)\n",
    "    tf.keras.layers.Dense(1) # 리얼 데이터인지 구별하는거라(진짜인지 아닌지 판별) -> binary classification\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_discriminator(outputs_real, outputs_fake):\n",
    "    # outputs_real: shape = (64(batch size), 1) - > real probability\n",
    "    # outputs_fake: shape = (64(batch size), 1) - > real probability\n",
    "    loss_1 = loss_ce(tf.ones_like(outputs_real), outputs_real) # 1로만 채워진 matrix, shape이 outputs real과 동일하도록\n",
    "    loss_2 = loss_ce(tf.zeros_like(outputs_fake), outputs_fake)\n",
    "    loss_all = loss_1 + loss_2\n",
    "    return loss_all\n",
    "\n",
    "def loss_generator(outputs_fake):\n",
    "    return loss_ce(tf.ones_like(outputs_fake), outputs_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gen = tf.keras.optimizers.Adam(rate_learning_generator)\n",
    "optimizer_dis = tf.keras.optimizers.Adam(rate_learning_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def step_train(images):\n",
    "    #images: 실제 이미지 (batch size, 28, 28, 1)\n",
    "    noise = tf.random.normal([size_batch, dim_noise]) # 정규분포\n",
    "    with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_dis:\n",
    "        images_gen = model_generator(noise, training = True) #(batch size, 28, 28, 1)\n",
    "        outputs_real = model_discriminator(images, training = True) #real probability(batch size, 1)\n",
    "        outputs_fake = model_discriminator(images_gen, training = True) #real probability(batch size, 1)\n",
    "        \n",
    "        loss_dis = loss_discriminator(outputs_real, outputs_fake)\n",
    "        loss_gen = loss_generator(outputs_fake)\n",
    "        \n",
    "    grad_gen = tape_gen.gradient(loss_gen, model_discriminator.trainable_weights)\n",
    "    grad_dis = tape_dis.gradient(loss_dis, model_discriminator.trainable_weights)\n",
    "    optimizer_dis.apply_gradients(zip(grad_dis, model_discriminator.trainable_weights))\n",
    "    optimizer_gen.apply_gradients(zip(grad_gen, model_discriminator.trainable_weights))\n",
    "\n",
    "    return loss_gen, loss_dis\n",
    "\n",
    "@tf.function\n",
    "def step_test(images):\n",
    "    noise = tf.random.normal([size_batch, dim_noise])\n",
    "    images_gen = model_generator(noise, training = False) #(batch size, 28, 28, 1)\n",
    "    outputs_real = model_discriminator(images, training = False) #real probability(batch size, 1)\n",
    "    outputs_fake = model_discriminator(images_gen, training = False) #real probability(batch size, 1)\n",
    "        \n",
    "    loss_dis = loss_discriminator(outputs_real, outputs_fake)\n",
    "    loss_gen = loss_generator(outputs_fake)\n",
    "\n",
    "    return loss_gen, loss_dis\n",
    "\n",
    "@tf.function\n",
    "def generate_images(num_displays):\n",
    "    noise = tf.random.normal([num_displays, dim_noise])\n",
    "    images_gen = model_generator(noise, training = False)\n",
    "\n",
    "    return images_generated\n",
    "\n",
    "def evaluate_dataset(dataset, training):\n",
    "    loss_gen = 0.0\n",
    "    loss_dis = 0.0\n",
    "    num_data = 0.0\n",
    "\n",
    "    for images_batch in dataset:\n",
    "        if training:\n",
    "            loss_gen_, loss_dis_ = step_train(images_batch)\n",
    "        else:\n",
    "            loss_gen_, loss_dis_ = step_test(images_batch)\n",
    "\n",
    "        loss_gen += loss_gen_.numpy() * images_batch.shape[0]\n",
    "        loss_dis += loss_dis_.numpy() * images_batch.shape[0]\n",
    "        num_data += images_batch.shape[0]\n",
    "\n",
    "    loss_gen /= num_data\n",
    "    loss_dis /= num_data\n",
    "\n",
    "    return loss_gen, loss_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] 1 epoch: loss_gen 0.5300 loss_disc 0.9101\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_dis_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b1917689fb61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloss_gen_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_disc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     print('[TEST] loss_gen {:.4f} loss_disc {:.4f}'.format(loss_gen_test,\n\u001b[1;32m----> 8\u001b[1;33m                                                            loss_dis_test))\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mind_epoch\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_dis_test' is not defined"
     ]
    }
   ],
   "source": [
    "for ind_epoch in range(0, num_epochs):\n",
    "    loss_gen_train, loss_dis_train = evaluate_dataset(dataset_train, True)\n",
    "    print('[TRAIN] {} epoch: loss_gen {:.4f} loss_disc {:.4f}'.format(ind_epoch + 1,\n",
    "        loss_gen_train, loss_dis_train))\n",
    "\n",
    "    loss_gen_test, loss_disc_test = evaluate_dataset(dataset_test, False)\n",
    "    print('[TEST] loss_gen {:.4f} loss_disc {:.4f}'.format(loss_gen_test,\n",
    "                                                           loss_dis_test))\n",
    "\n",
    "    if ind_epoch > 80:\n",
    "        images_generated = generate_images(num_displays)\n",
    "        images_generated = tf.squeeze(images_generated, axis=3)\n",
    "\n",
    "        show_images(images_generated, 'Generated by GAN')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
