{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "size_height = X_train.shape[1]\n",
    "size_width = X_train.shape[2]\n",
    "num_data_train = X_train.shape[0]\n",
    "num_data_test = X_test.shape[0]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "size_batch = 64\n",
    "num_classes = np.unique(Y_train).shape[0]\n",
    "\n",
    "rate_learning = 1e-3 ## 매우 중요\n",
    "rate_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dataset_train = dataset_train.shuffle(10000).batch(size_batch)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "dataset_test = dataset_test.batch(size_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 num_classes, size_kernel,\n",
    "                 num_filters, num_denses, rate_dropout):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes #int\n",
    "        self.size_kernel = size_kernel #int\n",
    "        self.num_filters = num_filters #list\n",
    "        self.num_denses = num_denses #list\n",
    "        self.rate_dropout = rate_dropout #float\n",
    "\n",
    "        ### layers\n",
    "        self.layer_conv_1 = tf.keras.layers.Conv2D(\n",
    "            num_filters[0], #output channel\n",
    "            (size_kernel, size_kernel), #kernel size \n",
    "            padding = 'same' #same size로 들어가면 same size로 만들어줌 / cf: valid 옵션을 주면 output이 줄어듬\n",
    "        ) # 3 * 3 * 1 * 64 (kernels) + 64(bias) = 10 * 64 = 640\n",
    "        self.layer_act_1 = tf.keras.layers.ReLU()\n",
    "        self.layer_pool_1 = tf.keras.layers.MaxPool2D(\n",
    "            strides = (2, 2),\n",
    "            pool_size = (2, 2),\n",
    "            padding = 'same'\n",
    "        )\n",
    "        \n",
    "        self.layer_conv_2 = tf.keras.layers.Conv2D(\n",
    "            num_filters[1], #output channel\n",
    "            (size_kernel, size_kernel), #kernel size \n",
    "            padding = 'same' #same size로 들어가면 same size로 만들어줌 / cf: valid 옵션을 주면 output이 줄어듬\n",
    "        ) # 3 * 3 * 64 * 128 (kernels) + 128(bias) =  * 128 = 73856\n",
    "        self.layer_act_2 = tf.keras.layers.ReLU()\n",
    "        self.layer_pool_2 = tf.keras.layers.MaxPool2D(\n",
    "            strides = (2, 2),\n",
    "            pool_size = (2, 2),\n",
    "            padding = 'same'\n",
    "        )\n",
    "        \n",
    "        self.layer_conv_3 = tf.keras.layers.Conv2D(\n",
    "            num_filters[2], #output channel\n",
    "            (size_kernel, size_kernel), #kernel size \n",
    "            padding = 'same' #same size로 들어가면 same size로 만들어줌 / cf: valid 옵션을 주면 output이 줄어듬\n",
    "        ) #3 * 3 * 128 * 256 (kernels) + 256(bias) =  * 256 = 73856\n",
    "        self.layer_act_3 = tf.keras.layers.ReLU()\n",
    "        self.layer_pool_3 = tf.keras.layers.MaxPool2D(\n",
    "            strides = (2, 2),\n",
    "            pool_size = (2, 2),\n",
    "            padding = 'same'\n",
    "        )\n",
    "        self.layer_flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.layer_fc_1 = tf.keras.layers.Dense(\n",
    "            num_denses[0], activation = tf.nn.relu\n",
    "        ) # 4 * 4 * 256 * 256 + 256(bias) = 1048832\n",
    "        \n",
    "        self.layer_fc_2 = tf.keras.layers.Dense(\n",
    "            num_denses[1], activation = tf.nn.relu\n",
    "        ) # 256 * 128 + 128 = 32896\n",
    "            \n",
    "        self.layer_fc_3 = tf.keras.layers.Dense(\n",
    "            num_classes, activation = tf.nn.softmax\n",
    "        ) # 128 * 10 + 10 = 1290  \n",
    "        \n",
    "        ###\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        ###\n",
    "        #inputs: (batch size, 28, 28, 1) , \n",
    "        outputs = inputs\n",
    "        # (batch size, 28, 28, 1)\n",
    "        \n",
    "        outputs = self.layer_conv_1(outputs)\n",
    "        # (batch size, 28, 28, 64)\n",
    "        outputs = self.layer_act_1(outputs)\n",
    "        # (batch size, 28, 28, 64)\n",
    "        outputs = self.layer_pool_1(outputs)\n",
    "        # (batch size, 14, 14, 64)\n",
    "        \n",
    "        outputs = self.layer_conv_2(outputs)\n",
    "        # (batch size, 14, 14, 128)\n",
    "        outputs = self.layer_act_2(outputs)\n",
    "        # (batch size, 14, 14, 128)\n",
    "        outputs = self.layer_pool_2(outputs)\n",
    "        # (batch size, 7, 7, 128)\n",
    "        \n",
    "        outputs = self.layer_conv_3(outputs)\n",
    "        # (batch size, 7, 7, 256)\n",
    "        outputs = self.layer_act_3(outputs)\n",
    "        # (batch size, 7, 7, 256)\n",
    "        outputs = self.layer_pool_3(outputs)\n",
    "        # (batch size, 4, 4, 256): 4차원 텐서\n",
    "        \n",
    "        outputs = self.layer_flatten(outputs)\n",
    "        # (batch size, 4* 4 * 256)\n",
    "        \n",
    "        outputs = self.layer_fc_1(outputs)\n",
    "        # (batch size, 256)\n",
    "        outputs = self.layer_fc_2(outputs)\n",
    "        # (batch size, 128)\n",
    "        outputs = self.layer_fc_3(outputs)\n",
    "        # (batch size, num_classes = 10)\n",
    "        # 숫자 0 ~ 9 분류\n",
    "        ###\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  640       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  73856     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  295168    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1048832   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 1,452,682\n",
      "Trainable params: 1,452,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "model = CNN(num_classes, \n",
    "            3, [64, 128, 256], \n",
    "            [256, 128], rate_dropout)\n",
    "optimizer = tf.keras.optimizers.Adam(rate_learning)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "###\n",
    "\n",
    "model.build((None, size_height, size_width, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator\n",
    "@tf.function\n",
    "def step_train(X, by):\n",
    "    ###\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds_ = model(X, True)\n",
    "        loss_ = loss(by, preds_)\n",
    "        \n",
    "    grads_ = tape.gradient(loss_, model.trainable_weights)\n",
    "    # dL/dw = gradient of w\n",
    "    # grads_ -> 1452682\n",
    "    optimizer.apply_gradients(\n",
    "        zip(grads_, model.trainable_weights)\n",
    "    )\n",
    "    \n",
    "    ###\n",
    "\n",
    "    return loss_\n",
    "\n",
    "@tf.function\n",
    "def step_test(X, by):\n",
    "    ###\n",
    "    preds_ = model(X, False)\n",
    "    loss_ = loss(by, preds_)\n",
    "    ###\n",
    "\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 EPOCH: loss_train 0.0400 acc_train 0.0000 loss_test 0.0366 acc_test 0.0000\n",
      "2 EPOCH: loss_train 0.0286 acc_train 0.0000 loss_test 0.0240 acc_test 0.0000\n",
      "3 EPOCH: loss_train 0.0218 acc_train 0.0000 loss_test 0.0225 acc_test 0.0000\n",
      "4 EPOCH: loss_train 0.0177 acc_train 0.0000 loss_test 0.0260 acc_test 0.0000\n",
      "5 EPOCH: loss_train 0.0152 acc_train 0.0000 loss_test 0.0310 acc_test 0.0000\n",
      "6 EPOCH: loss_train 0.0119 acc_train 0.0000 loss_test 0.0316 acc_test 0.0000\n",
      "7 EPOCH: loss_train 0.0111 acc_train 0.0000 loss_test 0.0303 acc_test 0.0000\n",
      "8 EPOCH: loss_train 0.0091 acc_train 0.0000 loss_test 0.0391 acc_test 0.0000\n",
      "9 EPOCH: loss_train 0.0090 acc_train 0.0000 loss_test 0.0374 acc_test 0.0000\n"
     ]
    }
   ],
   "source": [
    "for ind_epoch in range(0, num_epochs):\n",
    "    loss_train = 0.0\n",
    "    loss_test = 0.0\n",
    "\n",
    "    num_train = 0.0\n",
    "    num_test = 0.0\n",
    "\n",
    "    for ind_iter, (X_batch, by_batch) in enumerate(dataset_train):\n",
    "        # batch size = 64\n",
    "        ###\n",
    "        loss_ = step_train(X_batch, by_batch)\n",
    "        loss_train += loss_ * X_batch.shape[0]\n",
    "        num_train += X_batch.shape[0]\n",
    "        ###\n",
    "\n",
    "    for X_batch, by_batch in dataset_test:\n",
    "        # batch size = 63\n",
    "        ###\n",
    "        loss_ = step_test(X_batch, by_batch)\n",
    "        loss_test += loss_ * X_batch.shape[0]\n",
    "        num_test += X_batch.shape[0]\n",
    "        ###\n",
    "\n",
    "    loss_train /= num_train\n",
    "    loss_test /= num_test\n",
    "    \n",
    "    acc_train = 0.0\n",
    "    acc_test = 0.0\n",
    "\n",
    "    print('{} EPOCH: loss_train {:.4f} acc_train {:.4f} loss_test {:.4f} acc_test {:.4f}'.format(\n",
    "        ind_epoch + 1, loss_train, acc_train, loss_test, acc_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
