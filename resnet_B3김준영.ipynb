{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "size_height = X_train.shape[1]\n",
    "size_width = X_train.shape[2]\n",
    "num_data_train = X_train.shape[0]\n",
    "num_data_test = X_test.shape[0]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "size_batch = 64\n",
    "num_classes = np.unique(Y_train).shape[0]\n",
    "\n",
    "rate_learning = 1e-3\n",
    "rate_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dataset_train = dataset_train.shuffle(10000).batch(size_batch)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "dataset_test = dataset_test.batch(size_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_filters, downsampling):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "                \n",
    "        ### 채우기 시작 ###\n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters = num_filters,\n",
    "                                             kernel_size = (1, 1),\n",
    "                                            strides = 1,\n",
    "                                            padding = 'same')\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters = num_filters,\n",
    "                                             kernel_size = (3, 3),\n",
    "                                            strides = 1,\n",
    "                                            padding = 'same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters = num_filters * 4,\n",
    "                                             kernel_size = (1, 1),\n",
    "                                            strides = 1,\n",
    "                                            padding = 'same')\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.downsample = tf.keras.Sequential()\n",
    "        self.downsample.add(tf.keras.layers.Conv2D(filters = num_filters * 4,\n",
    "                                                  kernel_size = (1, 1),\n",
    "                                                  strides = 1))\n",
    "        self.downsample.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "        ### 채우기 끝 ###\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        ### 채우기 시작 ###\n",
    "        residual = self.downsample(inputs)\n",
    "        \n",
    "        outputs = self.conv2a(inputs)\n",
    "        outputs = self.bn2a(outputs, training = training)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        outputs = self.conv2b(outputs)\n",
    "        outputs = self.bn2b(outputs, training = training)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        outputs = self.conv2c(outputs)\n",
    "        outputs = self.bn2c(outputs, training = training)\n",
    "\n",
    "        outputs = tf.nn.relu(tf.keras.layers.add([residual, outputs]))\n",
    "        \n",
    "        ### 채우기 끝 ###\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_residual_block_layer(num_filters, num_blocks, downsampling):\n",
    "    block_residual = tf.keras.Sequential()\n",
    "    block_residual.add(ResidualBlock(num_filters, downsampling))\n",
    "\n",
    "    for _ in range(1, num_blocks):\n",
    "        block_residual.add(ResidualBlock(num_filters, False))\n",
    "\n",
    "    return block_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes, list_num_filters):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.layer_conv_1 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                                   kernel_size=(7, 7),\n",
    "                                                   strides=2,\n",
    "                                                   padding='same')\n",
    "        self.layer_bn_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.layer_act_1 = tf.keras.layers.ReLU()\n",
    "        self.layer_pool_1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\n",
    "                                                      strides=2,\n",
    "                                                      padding='same')\n",
    "\n",
    "        self.layer_block_1 = make_residual_block_layer(\n",
    "            num_filters=64,\n",
    "            num_blocks=list_num_filters[0],\n",
    "            downsampling=False\n",
    "        )\n",
    "        self.layer_block_2 = make_residual_block_layer(\n",
    "            num_filters=128,\n",
    "            num_blocks=list_num_filters[1],\n",
    "            downsampling=True\n",
    "        )\n",
    "        self.layer_block_3 = make_residual_block_layer(\n",
    "            num_filters=256,\n",
    "            num_blocks=list_num_filters[2],\n",
    "            downsampling=True\n",
    "        )\n",
    "        self.layer_block_4 = make_residual_block_layer(\n",
    "            num_filters=512,\n",
    "            num_blocks=list_num_filters[3],\n",
    "            downsampling=True\n",
    "        )\n",
    "\n",
    "        self.layer_pool_avg = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.layer_fc = tf.keras.layers.Dense(num_classes,\n",
    "                                              activation=tf.nn.softmax)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        outputs = inputs\n",
    "        \n",
    "        outputs = self.layer_conv_1(outputs)\n",
    "        outputs = self.layer_bn_1(outputs, training=training)\n",
    "        outputs = self.layer_act_1(outputs)\n",
    "        outputs = self.layer_pool_1(outputs)\n",
    "        outputs = self.layer_block_1(outputs, training=training)\n",
    "        outputs = self.layer_block_2(outputs, training=training)\n",
    "        outputs = self.layer_block_3(outputs, training=training)\n",
    "        outputs = self.layer_block_4(outputs, training=training)\n",
    "        outputs = self.layer_pool_avg(outputs)\n",
    "        outputs = self.layer_fc(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"res_net_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            multiple                  3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch multiple                  256       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    multiple                  215296    \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    multiple                  930304    \n",
      "_________________________________________________________________\n",
      "sequential_8 (Sequential)    multiple                  3695616   \n",
      "_________________________________________________________________\n",
      "sequential_11 (Sequential)   multiple                  14731264  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  20490     \n",
      "=================================================================\n",
      "Total params: 19,596,426\n",
      "Trainable params: 19,557,898\n",
      "Non-trainable params: 38,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(num_classes, [2, 2, 2, 2])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=rate_learning)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "metric_train = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "metric_test = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model.build((None, size_height, size_width, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def step_train(X, by):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds_ = model(X, True)\n",
    "        loss_ = loss(by, preds_)\n",
    "\n",
    "    grads_ = tape.gradient(loss_, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads_, model.trainable_weights))\n",
    "    metric_train.update_state(by, preds_)\n",
    "\n",
    "    return loss_\n",
    "\n",
    "@tf.function\n",
    "def step_test(X, by):\n",
    "    preds_ = model(X, False)\n",
    "    loss_ = loss(by, preds_)\n",
    "\n",
    "    metric_test.update_state(by, preds_)\n",
    "\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind_epoch in range(0, num_epochs):\n",
    "    loss_train = 0.0\n",
    "    loss_test = 0.0\n",
    "\n",
    "    num_train = 0.0\n",
    "    num_test = 0.0\n",
    "\n",
    "    for ind_iter, (X_batch, by_batch) in enumerate(dataset_train):\n",
    "        loss_ = step_train(X_batch, by_batch)\n",
    "        loss_train += loss_ * X_batch.shape[0]\n",
    "        num_train += X_batch.shape[0]\n",
    "\n",
    "    loss_train /= num_train\n",
    "\n",
    "    acc_train = metric_train.result()\n",
    "    metric_train.reset_states()\n",
    "\n",
    "    for X_batch, by_batch in dataset_test:\n",
    "        loss_ = step_test(X_batch, by_batch)\n",
    "        loss_test += loss_ * X_batch.shape[0]\n",
    "        num_test += X_batch.shape[0]\n",
    "\n",
    "    loss_test /= num_test\n",
    "\n",
    "    acc_test = metric_test.result()\n",
    "    metric_test.reset_states()\n",
    "\n",
    "    print('{} EPOCH: loss_train {:.4f} acc_train {:.4f} loss_test {:.4f} acc_test {:.4f}'.format(\n",
    "        ind_epoch + 1, loss_train, acc_train, loss_test, acc_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
